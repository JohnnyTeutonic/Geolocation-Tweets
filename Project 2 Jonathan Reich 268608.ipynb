{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/jonathan/anaconda3/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/jonathan/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "    /Users/jonathan/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.classify import NaiveBayesClassifier as nbc\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "import gensim\n",
    "import operator\n",
    "import os\n",
    "# for 'kernel crashes' using xgboost...\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import spacy\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc, average_precision_score, make_scorer, f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "from gensim import corpora\n",
    "punctuations = string.punctuation\n",
    "stopwords = stopwords.words('english')\n",
    "!python -m spacy download en \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import one_hot, text_to_word_sequence, hashing_trick, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from xgboost import XGBClassifier\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the raw training data\n",
    "train_raw = pd.read_csv('data/train-raw.tsv', sep='\\t')\n",
    "train_raw_x = train_raw['Text']\n",
    "train_raw_y = train_raw['Location']\n",
    "# read in the dev data\n",
    "dev_test = pd.read_csv('data/dev-raw.tsv', sep='\\t', skiprows=1, header=None)\n",
    "dev_test.columns = ['ID', 'Location', 'Text']\n",
    "dev_test_x = dev_test['Text']\n",
    "dev_test_y = dev_test['Location']\n",
    "# read in the test data\n",
    "final_test = pd.read_csv('data/test-raw.tsv', sep='\\t', skiprows=1, header=None)\n",
    "final_test.columns = ['ID', 'Location', 'Text']\n",
    "final_test_x = final_test['Text']\n",
    "final_test_y = final_test['Location']\n",
    "\n",
    "# perform preprocessing step including removing non-alphanumeric characters, converting all words into their 'base' form\n",
    "# (using stemming), and conveting all words to lowercase.\n",
    "trim = PorterStemmer()\n",
    "train_cleaned_x = train_raw_x.apply(lambda x: \" \".join([trim.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in stopwords]).lower())\n",
    "dev_cleaned_x = dev_test_x.apply(lambda x: \" \".join([trim.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in stopwords]).lower())\n",
    "test_cleaned_x = final_test_x.apply(lambda x: \" \".join([trim.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in stopwords]).lower())\n",
    "# create a tf-idf vectorizer (these parameters have been set based on testing over a substantial period of time)\n",
    "# we will use both unigrams, bigrams and trigrams in this model/\n",
    "vectorizer = TfidfVectorizer(min_df= 10,max_df=0.5, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1038eef28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this shows that the class distribution for each city is invariant.\n",
    "train_raw.groupby('Location').Location.count().plot.bar(ylim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is: RandomForestClassifier and accuracy is: 0.30469503698145567 \n",
      "model is: GradientBoostingClassifier and accuracy is: 0.3082591917676064 \n",
      "model is: MultinomialNB and accuracy is: 0.30732125629756674 \n",
      "model is: LinearSVC and accuracy is: 0.26246114267338405 \n",
      "model is: KNeighborsClassifier and accuracy is: 0.24975881659341836 \n",
      "model is: DecisionTreeClassifier and accuracy is: 0.3046682388251688 \n"
     ]
    }
   ],
   "source": [
    "# try the candidate models on the preprocessed data given to us.\n",
    "df100 = dd.read_csv('data/train-top100.csv')\n",
    "train_100 = dd.read_csv('data/train-top100.csv')\n",
    "train_100_x = train_100.loc[:, train_100.columns != 'Location']\n",
    "train_100_y = train_100.loc[:, 'Location']\n",
    "dev_100 = dd.read_csv('data/dev-top100.csv')\n",
    "dev_100_x = dev_100.loc[:, dev_100.columns != 'Location']\n",
    "dev_100_y = dev_100.loc[:, 'Location']\n",
    "model_list = [RandomForestClassifier(), GradientBoostingClassifier(), MultinomialNB(), LinearSVC(), \n",
    "             KNeighborsClassifier(n_neighbors=4), DecisionTreeClassifier(), XGBClassifier()]\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    for mod in model_list:\n",
    "        name = mod.__str__().split('(')[0]\n",
    "        mod.fit(train_100_x, train_100_y)\n",
    "        print(\"model is: {0} and accuracy is: {1} \".format(name, accuracy_score(mod.predict(dev_100_x), dev_100_y)))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is: RandomForestClassifier and accuracy is: 0.3231635388739946 \n",
      "model is: GradientBoostingClassifier and accuracy is: 0.30498659517426274 \n",
      "model is: MultinomialNB and accuracy is: 0.32756032171581767 \n",
      "model is: LinearSVC and accuracy is: 0.3253619302949062 \n",
      "model is: KNeighborsClassifier and accuracy is: 0.27249329758713137 \n",
      "model is: DecisionTreeClassifier and accuracy is: 0.31654155495978553 \n",
      "model is: XGBClassifier and accuracy is: 0.303887399463807 \n"
     ]
    }
   ],
   "source": [
    "# iterate through a list of candidate classifiers using their (mostly) default settings.\n",
    "# this will take a long time to do. The output of this step was used to help select from these initial candidate models.\n",
    "model_list = [RandomForestClassifier(), GradientBoostingClassifier(), MultinomialNB(), LinearSVC(), \n",
    "             KNeighborsClassifier(n_neighbors=7), DecisionTreeClassifier(), XGBClassifier()]\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    for mod in model_list:\n",
    "        name = mod.__str__().split('(')[0]\n",
    "        pipeline_vect = Pipeline([('vect', vectorizer),\n",
    "                     ('mod', mod)])\n",
    "        preds_vect = pipeline_vect.fit(train_cleaned_x, train_raw_y).predict(dev_cleaned_x)\n",
    "        print(\"model is: {0} and accuracy is: {1} \".format(name, accuracy_score(preds_vect, dev_test_y)))\n",
    "\n",
    "# n.b. that our vectorizer performs better than the preprocessed data given in the inital dataset (see performance of\n",
    "# the models above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:  1.0min remaining:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.3min finished\n",
      "/Users/jonathan/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31450402144772116\n"
     ]
    }
   ],
   "source": [
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 200, 10)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [4, 7, 15, 23]\n",
    "min_samples_leaf = [3, 5, 10, 15]\n",
    "# grid of different feature values used to help fine-tune the hyperparameters.\n",
    "random_grid = {\n",
    "               'rf__max_features': max_features,\n",
    "               'rf__max_depth': max_depth,\n",
    "               'rf__min_samples_split': min_samples_split,\n",
    "               'rf__min_samples_leaf': min_samples_leaf}\n",
    "rfpipe = Pipeline([ ('vect', vectorizer), ('rf', RandomForestClassifier())])\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator=rfpipe, param_distributions= random_grid, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_fitted_model = rf_random.fit(train_cleaned_x, train_raw_y)\n",
    "preds_rf_opt = rf_fitted_model.predict(dev_cleaned_x)\n",
    "\n",
    "print(accuracy_score(preds_rf_opt, dev_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is quite slow - it is used for hyperparameter tuning for the SVC, with no benefit gleaned.\n",
    "\n",
    "SVCpipe = Pipeline([ ('vect', vectorizer), ('SVC', LinearSVC())])\n",
    "\n",
    "parameters = {'SVC__C':np.arange(0.01,100,10)}\n",
    "linearSVC = RandomizedSearchCV(SVCpipe,parameters,cv=3,return_train_score=True, n_jobs=-1, verbose=2)\n",
    "preds_svc_opt = linearSVC.fit(train_cleaned_x, train_raw_y).predict(dev_cleaned_x)\n",
    "print(accuracy_score(preds_svc_opt, dev_test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32756032171581767\n"
     ]
    }
   ],
   "source": [
    "# this 'out-of-the-box' MNB model was used in the kaggle predictions.\n",
    "\n",
    "pipeline_vect = Pipeline([('vect', vectorizer),\n",
    "                     ('clf_nb', MultinomialNB())])\n",
    "preds_vect_nb = pipeline_vect.fit(train_cleaned_x, train_raw_y)\n",
    "dev_preds = preds_vect_nb.predict(dev_cleaned_x)\n",
    "\n",
    "print(accuracy_score(dev_preds, dev_test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32734584450402143\n"
     ]
    }
   ],
   "source": [
    "# initial hyperparameter tuning of this model didn't yield a better accuracy than above:\n",
    "\n",
    "pipeline = Pipeline([('vec', vectorizer),\n",
    "             ('clf',   MultinomialNB())])\n",
    "parameters = {  \n",
    "'vec__max_df': (0.3, 0.5, 0.7, 1.0),  \n",
    "'vec__max_features': (None, 1500, 1700),  \n",
    "'vec__min_df': (2, 3, 5, 7, 10, 20),  \n",
    "'vec__norm' : ('l1', 'l2'),\n",
    "'vec__ngram_range': ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001)  \n",
    "}  \n",
    "\n",
    "grid_search = RandomizedSearchCV(pipeline, parameters, n_jobs=-1, verbose=2, cv=5)  \n",
    "fitted_grid = grid_search.fit(train_cleaned_x, train_raw_y)\n",
    "preds_vect = fitted_grid.predict(dev_cleaned_x)\n",
    "print(accuracy_score(preds_vect, dev_test_y))\n",
    "best_parameters = fitted_grid.best_estimator_.get_params()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning using a more-exhausting gridsearch strategy on the gradient boosting classifer.\n",
    "# this step can take up to an hour to process.\n",
    "\n",
    "gradpipe = Pipeline([('vect', vectorizer), ('clf', GradientBoostingClassifier())])\n",
    "params = {'clf__n_estimators': [500, 700], 'clf__max_depth': [8, 20],\n",
    "          'clf__learning_rate': [0.01, 0.001]}\n",
    "\n",
    "gridboost = GridSearchCV(gradpipe,params,cv=2, return_train_score=True, n_jobs=-1, verbose=2)\n",
    "fitted_grid = gridboost.fit(train_cleaned_x, train_raw_y).predict(dev_cleaned_x)\n",
    "preds_vect = fitted_grid.predict(dev_cleaned_x)\n",
    "print(accuracy_score(preds_vect, dev_test_y))\n",
    "best_parameters = fitted_grid.best_estimator_.get_params()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Brisbane       0.32      0.34      0.33      9324\n",
      "   Melbourne       0.33      0.34      0.33      9325\n",
      "       Perth       0.34      0.28      0.31      9325\n",
      "      Sydney       0.32      0.35      0.33      9326\n",
      "\n",
      "   micro avg       0.33      0.33      0.33     37300\n",
      "   macro avg       0.33      0.33      0.33     37300\n",
      "weighted avg       0.33      0.33      0.33     37300\n",
      "\n",
      "[[3212 2080 1739 2293]\n",
      " [2154 3159 1716 2296]\n",
      " [2319 2198 2624 2184]\n",
      " [2290 2112 1701 3223]]\n"
     ]
    }
   ],
   "source": [
    "# the confusion matrix shows that there is little difference in the performance of the gradient boosting classifier\n",
    "# on all four cities.\n",
    "\n",
    "print(classification_report(dev_test_y,preds_vect))\n",
    "print(confusion_matrix(dev_test_y, preds_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree classifier hyper-parameteter tuning with five-fold cross-validation.\n",
    "parameters={'min_samples_split' : np.arange(10,700,20),'max_depth': np.arange(1,100,2), 'criterion': ('gini', 'entropy')}\n",
    "\n",
    "dctpipe = make_pipeline((vectorizer), GridSearchCV(DecisionTreeClassifier(), n_jobs=-1, verbose=2, cv=5, param_grid=parameters, refit=True))\n",
    "\n",
    "\n",
    "optimised_dtc = dctpipe.fit(train_cleaned_x ,train_raw_y).predict(dev_cleaned_x)\n",
    "pprint(accuracy_score(optimised_dtc, ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jonathan/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/jonathan/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/jonathan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 77520 samples, validate on 25840 samples\n",
      "Epoch 1/15\n",
      "77520/77520 [==============================] - 7s 92us/step - loss: 11.8668 - acc: 0.2515 - val_loss: 11.4927 - val_acc: 0.2547\n",
      "Epoch 2/15\n",
      "77520/77520 [==============================] - 6s 82us/step - loss: 11.8664 - acc: 0.2519 - val_loss: 11.5508 - val_acc: 0.2521\n",
      "Epoch 3/15\n",
      "77520/77520 [==============================] - 6s 81us/step - loss: 11.8993 - acc: 0.2505 - val_loss: 11.6830 - val_acc: 0.2493\n",
      "Epoch 4/15\n",
      "77520/77520 [==============================] - 7s 90us/step - loss: 11.9375 - acc: 0.2492 - val_loss: 11.6371 - val_acc: 0.2504\n",
      "Epoch 5/15\n",
      "77520/77520 [==============================] - 7s 86us/step - loss: 11.9073 - acc: 0.2508 - val_loss: 11.6504 - val_acc: 0.2507\n",
      "Epoch 6/15\n",
      "77520/77520 [==============================] - 7s 92us/step - loss: 11.8928 - acc: 0.2514 - val_loss: 11.6061 - val_acc: 0.2473\n",
      "Epoch 7/15\n",
      "77520/77520 [==============================] - 7s 86us/step - loss: 11.8768 - acc: 0.2517 - val_loss: 11.6031 - val_acc: 0.2487\n",
      "Epoch 8/15\n",
      "77520/77520 [==============================] - 6s 82us/step - loss: 11.8423 - acc: 0.2539 - val_loss: 11.4641 - val_acc: 0.2612\n",
      "Epoch 9/15\n",
      "77520/77520 [==============================] - 6s 80us/step - loss: 11.8638 - acc: 0.2537 - val_loss: 11.4931 - val_acc: 0.2599\n",
      "Epoch 10/15\n",
      "77520/77520 [==============================] - 8s 99us/step - loss: 11.8567 - acc: 0.2533 - val_loss: 11.4760 - val_acc: 0.2612\n",
      "Epoch 11/15\n",
      "77520/77520 [==============================] - 6s 82us/step - loss: 11.8607 - acc: 0.2539 - val_loss: 11.4632 - val_acc: 0.2623\n",
      "Epoch 12/15\n",
      "77520/77520 [==============================] - 6s 78us/step - loss: 11.8340 - acc: 0.2552 - val_loss: 11.4788 - val_acc: 0.2611\n",
      "Epoch 13/15\n",
      "77520/77520 [==============================] - 6s 79us/step - loss: 11.8363 - acc: 0.2550 - val_loss: 11.4735 - val_acc: 0.2582\n",
      "Epoch 14/15\n",
      "77520/77520 [==============================] - 6s 78us/step - loss: 11.8247 - acc: 0.2548 - val_loss: 11.5105 - val_acc: 0.2547\n",
      "Epoch 15/15\n",
      "77520/77520 [==============================] - 6s 79us/step - loss: 11.8257 - acc: 0.2544 - val_loss: 11.5292 - val_acc: 0.2522\n",
      "77520/77520 [==============================] - 1s 13us/step\n",
      "Training Accuracy: 0.2527\n"
     ]
    }
   ],
   "source": [
    "# This is the initial MLP model with architecture FC layer (Dense) of 100 neurons, then a final four neuron layer\n",
    "# for each city/class. \n",
    "\n",
    "sentences = train_cleaned_x.values\n",
    "y = train_raw_y.values\n",
    "\n",
    "# convert this into 'one-hot encoded' form (necessary for keras models)\n",
    "onehot_y = pd.get_dummies(y)\n",
    "target_labels_y = onehot_y.columns\n",
    "y_mat = onehot_y.as_matrix()\n",
    "\n",
    "# split the training data into train/test splits\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y_mat, test_size=0.25, random_state=1000)\n",
    "\n",
    "# tokenize the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "# ensure each data point is of length 30.\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=30)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=30)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "\n",
    "# this is the model used for the first MLP\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# perform dropout to try and prevent over-fitting.\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# ensure the 'categorical crossentropy' is used as this is a multi-class problem.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit the data, validate against the held-out training data\n",
    "fitted_model = model.fit(np.array(X_train), np.array(y_train),\n",
    "                    epochs=20,\n",
    "                    verbose=True,\n",
    "                    validation_data=(np.array(X_test), np.array(y_test)),\n",
    "                    batch_size=20)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=True)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "# save the model\n",
    "model.save('model_keras_updated.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell contains an alternative model used for the MLP used in the kaggle predictions step.\n",
    "\n",
    "sentences = train_cleaned_x.values\n",
    "y = train_raw_y.values\n",
    "\n",
    "onehot_y = pd.get_dummies(y)\n",
    "target_labels_y = onehot_y.columns\n",
    "y_mat = onehot_y.as_matrix()\n",
    "\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y_mat, test_size=0.20, random_state=1000)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=20)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=20)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "# improved architecture uses a preprocessed embedding vector (of dimension size 50)\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=20))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(Dense(128, input_shape = (vocab_size,), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(np.array(X_train), np.array(y_train),\n",
    "                    epochs=11,\n",
    "                    verbose=True,\n",
    "                    validation_data=(np.array(X_test), np.array(y_test)),\n",
    "                    batch_size=20)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=True)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=True)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "model.save('model_keras_alt.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20672, 20), (82688, 20))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_test).shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev accuracy for first model is:  0.5135656836461125\n",
      "dev accuracy for second model is:  0.23292225201072386\n"
     ]
    }
   ],
   "source": [
    "sentences = train_cleaned_x.values\n",
    "y = train_raw_y.values\n",
    "\n",
    "onehot_y = pd.get_dummies(y)\n",
    "target_labels_y = onehot_y.columns\n",
    "y_mat = onehot_y.as_matrix()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "# load the initial model\n",
    "fin_mod_mlp = load_model('model_keras_updated.h5')\n",
    "# load the alternative mlp model\n",
    "fin_mod_alt = load_model('model_keras_alt.h5')\n",
    "\n",
    "final_sentences = test_cleaned_x.values\n",
    "final_preds_seq = tokenizer.texts_to_sequences(final_sentences)\n",
    "final_preds_seq_alt =  pad_sequences(final_preds_seq, padding='post', maxlen=20)\n",
    "final_preds_seq =  pad_sequences(final_preds_seq, padding='post', maxlen=30)\n",
    "\n",
    "# final predictions on the test data:\n",
    "# first model\n",
    "preds_fin_mlp = fin_mod_mlp.predict(final_preds_seq)\n",
    "# second model\n",
    "preds_fin_alt = fin_mod_alt.predict(final_preds_seq_alt)\n",
    "\n",
    "dev_sentences = dev_cleaned_x.values\n",
    "dev_test = tokenizer.texts_to_sequences(dev_sentences)\n",
    "# pad out the sentence to ensure they are of the correct length.\n",
    "dev_test_alt = pad_sequences(dev_test, padding='post', maxlen=20)\n",
    "# minor difference used for alternative mlp (different sentence length)\n",
    "dev_test =  pad_sequences(dev_test, padding='post', maxlen=30)\n",
    "\n",
    "# predictions on the dev data for both models\n",
    "preds_dev_mlp = fin_mod_mlp.predict(dev_test)\n",
    "preds_dev_mlp_alt = fin_mod_alt.predict(dev_test_alt)\n",
    "\n",
    "# obtain the dev accuracy for first mod\n",
    "samples = dev_test_x.shape[0]\n",
    "true=0\n",
    "for c,d in zip(preds_dev_mlp, dev_test_y):\n",
    "    if np.argmax(c)==np.argmax(d):\n",
    "        true+=1\n",
    "acc_mlp = true/samples\n",
    "\n",
    "# obtain the dev accuracy for alternative mlp model\n",
    "true_alt=0\n",
    "for c,d in zip(preds_dev_mlp_alt, dev_test_y):\n",
    "    if np.argmax(c)==np.argmax(d):\n",
    "        true_alt+=1\n",
    "acc_alt = true_alt/samples\n",
    "\n",
    "print(\"dev accuracy for first model is: \", acc_mlp)\n",
    "print(\"dev accuracy for second model is: \", acc_alt)\n",
    "\n",
    "# note the huge difference between training and dev accuracy below for the second model. this model has *hugely\n",
    "# overfit the data (had a training accuracy of 98%, dev accuracy of 23%, which is worse than a zeroR baseline.)\n",
    "\n",
    "# the first model gave better test predictions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate the keras predictions back into correct form for kaggle predictions.\n",
    "preds_fin_one = np.zeros(shape=preds_fin_mlp.shape)\n",
    "\n",
    "# obtain the largest probability for each data point:\n",
    "for row, col in enumerate(preds_fin_mlp):\n",
    "    p = np.argmax(col)\n",
    "    preds_fin_one[row, p] = 1\n",
    "    \n",
    "# create a list to hold the final predictions:\n",
    "preds_map = [None for _ in range(preds_fin_one.shape[0])]\n",
    "\n",
    "# Change the 'ones' into strings:\n",
    "for x in range(0, preds_fin_one.shape[0]):\n",
    "    for y in range(0, preds_fin_one.shape[1]):\n",
    "        if preds_fin_one[x, 0] == 1: \n",
    "            preds_map[x] = 'Brisbane'\n",
    "            break\n",
    "        if preds_fin_one[x, 1] == 1: \n",
    "            preds_map[x] = 'Melbourne'\n",
    "            break\n",
    "        if preds_fin_one[x, 2] == 1:\n",
    "            preds_map[x] = 'Perth'\n",
    "            break\n",
    "        if preds_fin_one[x, 3] == 1:\n",
    "            preds_map[x] = 'Sydney'\n",
    "            break    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this kaggle submission holds the final MLP model\n",
    "kaggle_submission = pd.DataFrame(columns=['ID', 'Class'])\n",
    "kaggle_submission.ID = final_test.ID\n",
    "kaggle_submission.Class = preds_map\n",
    "\n",
    "kaggle_submission.to_csv('kaggle_preds_mlp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this kaggle submission holds the multinomial naive bayes model\n",
    "preds_fin_vect = pipeline_vect.fit(train_cleaned_x, train_raw_y).predict(test_cleaned_x.values)\n",
    "kaggle_submission = pd.DataFrame(columns=['ID', 'Class'])\n",
    "kaggle_submission.ID = final_test.ID\n",
    "kaggle_submission.Class = preds_vect_nb.predict(test_cleaned_x.values)\n",
    "kaggle_submission.to_csv('kaggle_preds_nb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
